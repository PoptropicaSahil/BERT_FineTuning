{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX","timestamp":1666081953695},{"file_id":"1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU","timestamp":1590604015708},{"file_id":"1FsBCkREOaDopLF3PIYUuQxLR8wRfjQY1","timestamp":1559844903389},{"file_id":"1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y","timestamp":1554843110227}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2e1033a970b04817be02ed09dbd58fcd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a97c5d06701d4fbd92e2d7af0f53a4e7","IPY_MODEL_a79c4d6f3ff14d5aa948ec36e82fc904","IPY_MODEL_5ea9ef80668642939b6861c6ee92ebf1"],"layout":"IPY_MODEL_e1dc8d62ae784d46a810525569e9200e"}},"a97c5d06701d4fbd92e2d7af0f53a4e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf8a2d5719684138ae4dd13fafab53df","placeholder":"​","style":"IPY_MODEL_92de8e478b144841bae291ef2a897024","value":"Downloading: 100%"}},"a79c4d6f3ff14d5aa948ec36e82fc904":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41e0018db87c4c5abb475172c5db2530","max":507,"min":0,"orientation":"horizontal","style":"IPY_MODEL_def1ae5ca7834922b9062af32e80e2ff","value":507}},"5ea9ef80668642939b6861c6ee92ebf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaed1338cbae4630963d59caf3a4b697","placeholder":"​","style":"IPY_MODEL_59abb58fbc4146e6bcd8c7a4b563fb7b","value":" 507/507 [00:00&lt;00:00, 11.6kB/s]"}},"e1dc8d62ae784d46a810525569e9200e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf8a2d5719684138ae4dd13fafab53df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92de8e478b144841bae291ef2a897024":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41e0018db87c4c5abb475172c5db2530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"def1ae5ca7834922b9062af32e80e2ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaed1338cbae4630963d59caf3a4b697":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59abb58fbc4146e6bcd8c7a4b563fb7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16c62e7badb741b09cd4e08ee7ab324f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4070a095b0f443d4897013b931474e85","IPY_MODEL_c50650314249408ea7c405e8d1c511bf","IPY_MODEL_942ce4e0f4e84bca99e076cad4c3807d"],"layout":"IPY_MODEL_1166aef45d9e4350bd84919ec01c6ec5"}},"4070a095b0f443d4897013b931474e85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bfb3d0300e14b5688b9302bb8d63a7d","placeholder":"​","style":"IPY_MODEL_00954618803a4649b407a5294d352d32","value":"Downloading: 100%"}},"c50650314249408ea7c405e8d1c511bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6eb54771d2914654bc67118b55b33abb","max":5646064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_387274ac85f44a3ba9ac43acf44d059b","value":5646064}},"942ce4e0f4e84bca99e076cad4c3807d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a669d5b492842fca6638cdacfab401a","placeholder":"​","style":"IPY_MODEL_fc6dcb04a2c94ad782ce9a898c41341b","value":" 5.65M/5.65M [00:00&lt;00:00, 6.30MB/s]"}},"1166aef45d9e4350bd84919ec01c6ec5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bfb3d0300e14b5688b9302bb8d63a7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00954618803a4649b407a5294d352d32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eb54771d2914654bc67118b55b33abb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"387274ac85f44a3ba9ac43acf44d059b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a669d5b492842fca6638cdacfab401a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc6dcb04a2c94ad782ce9a898c41341b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65f2d95e43ca48f9bcf81cefc1f76346":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d13841ddad94dedbce5197e3b7fbae4","IPY_MODEL_4fc6c3645ad04e41abb088cf46a616ee","IPY_MODEL_455e4abcd6fb40628e04c2adecbe8065"],"layout":"IPY_MODEL_d0fd53a246784cd89e5ada05bff0fd9b"}},"4d13841ddad94dedbce5197e3b7fbae4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68a46bed388a40639d0d87b9b43de744","placeholder":"​","style":"IPY_MODEL_9c5e89392905469e953776b9b4d66352","value":"Downloading: 100%"}},"4fc6c3645ad04e41abb088cf46a616ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3de637f4bd244def8b616f303ca68fe9","max":134982446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa27c5508a9742dbad44c4f4b9acf710","value":134982446}},"455e4abcd6fb40628e04c2adecbe8065":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9717ca73c3a42dc9f65f4076690a88c","placeholder":"​","style":"IPY_MODEL_e17894005f484f349b5f6101e4b4e92e","value":" 135M/135M [00:03&lt;00:00, 45.7MB/s]"}},"d0fd53a246784cd89e5ada05bff0fd9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a46bed388a40639d0d87b9b43de744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c5e89392905469e953776b9b4d66352":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3de637f4bd244def8b616f303ca68fe9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa27c5508a9742dbad44c4f4b9acf710":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9717ca73c3a42dc9f65f4076690a88c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e17894005f484f349b5f6101e4b4e92e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"78HE8FLsKN9Q"},"source":["In this post, I take an in-depth look at word embeddings produced by Google's BERT and show you how to get started with BERT by producing your own word embeddings.\n","\n","This post is presented in two forms--as a blog post [here](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/) and as a Colab notebook [here](https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX). \n","The content is identical in both, but: \n","\n","* The blog post format may be easier to read, and includes a comments section for discussion. \n","* The Colab Notebook will allow you to run the code and inspect it as you read through.\n","\n","*UPDATE: May 27, 2020 - I've updated this post to use the new `transformers` library  from huggingface in place of the old `pytorch-pretrained-bert` library. You can still find the old post / Notebook [here](https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU) if you need it.* \n"]},{"cell_type":"markdown","metadata":{"id":"dYapTjoYa0kO"},"source":["# Introduction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c8HDKzBai5dL"},"source":["### History\n","\n","2018 was a breakthrough year in NLP. Transfer learning, particularly models like Allen AI's ELMO, OpenAI's Open-GPT, and Google's BERT allowed researchers to smash multiple benchmarks with minimal task-specific fine-tuning and provided the rest of the NLP community with pretrained models that could easily (with less data and less compute time) be fine-tuned and implemented to produce state of the art results. Unfortunately, for many starting out in NLP and even for some experienced practicioners, the theory and practical application of these powerful models is still not well understood.\n"]},{"cell_type":"markdown","metadata":{"id":"WoitNQMWA1bt"},"source":["\n","### What is BERT?\n","\n","BERT (Bidirectional Encoder Representations from Transformers), released in late 2018, is the model we will use in this tutorial to provide readers with a better understanding of and practical guidance for using transfer learning models in NLP. BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"q-dDVmXAA3At"},"source":["\n","### Why BERT embeddings?\n","\n","In this tutorial, we will use BERT to extract features, namely word and sentence embedding vectors, from text data. What can we do with these word and sentence embedding vectors? First, these embeddings are useful for keyword/search expansion, semantic search and information retrieval. For example, if you want to match customer questions or searches against already answered questions or well documented searches, these representations will help you accuratley retrieve results matching the customer's intent and contextual meaning, even if there's no keyword  or phrase overlap.\n","\n","Second, and perhaps more importantly, these vectors are used as high-quality feature inputs to downstream models. NLP models such as LSTMs or CNNs require inputs in the form of numerical vectors, and this typically means translating features like the vocabulary and parts of speech into numerical representations. In the past, words have been represented either as uniquely indexed values (one-hot encoding), or more helpfully as neural word embeddings where vocabulary words are matched against the fixed-length feature embeddings that result from models like Word2Vec or Fasttext. BERT offers an advantage over models like Word2Vec, because while each word has a fixed representation under Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. For example, given two sentences:\n","\n","\"The man was accused of robbing a bank.\"\n","\"The man went fishing by the bank of the river.\"\n","\n","Word2Vec would produce the same word embedding for the word \"bank\" in both sentences, while under BERT the word embedding for \"bank\" would be different for each sentence. Aside from capturing obvious differences like polysemy, the context-informed word embeddings capture other forms of information that result in more accurate feature representations, which in turn results in better model performance.\n","\n","From an educational standpoint, a close examination of BERT word embeddings is a good way to get  your feet wet with BERT and its family of transfer learning models, and sets us up with some practical knowledge and context to better understand the inner details of the model in later tutorials.\n","\n","Onward!"]},{"cell_type":"markdown","metadata":{"id":"Pqa-7WXBAw8q"},"source":["# 1. Loading Pre-Trained BERT"]},{"cell_type":"markdown","metadata":{"id":"eCdqJCtQN52l"},"source":["Install the pytorch interface for BERT by Hugging Face. (This library contains interfaces for other pretrained language models like OpenAI's GPT and GPT-2.) \n","\n","We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","If you're running this code on Google Colab, you will have to install this library each time you reconnect; the following cell will take care of that for you."]},{"cell_type":"code","metadata":{"id":"1RfUN_KolV-f","executionInfo":{"status":"ok","timestamp":1666590834798,"user_tz":-330,"elapsed":12064,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"829e6b34-4125-4c56-edce-e4c9e13a182f"},"source":["!pip install transformers --quiet"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 5.3 MB 6.3 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 44.5 MB/s \n","\u001b[K     |████████████████████████████████| 163 kB 63.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","metadata":{"id":"JSXImOxMPdNg"},"source":["Now let's import pytorch, the pretrained BERT model, and a BERT tokenizer. \n","\n","We'll explain the BERT model in detail in a later tutorial, but this is the pre-trained model released by Google that ran for many, many hours on Wikipedia and [Book Corpus](https://arxiv.org/pdf/1506.06724.pdf), a dataset containing +10,000 books of different genres. This model is responsible (with a little modification) for beating NLP benchmarks across a range of tasks. Google released a few variations of BERT models, but the one we'll use here is the smaller of the two available sizes (\"base\" and \"large\") and ignores casing, hence \"uncased.\"\"\n","\n","`transformers` provides a number of classes for applying BERT to different tasks (token classification, text classification, ...). Here, we're using the basic `BertModel` which has no specific output task--it's a good choice for using BERT just to extract embeddings."]},{"cell_type":"code","source":["!pip install sentencepiece --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8KUHmUErX5Mm","executionInfo":{"status":"ok","timestamp":1666590840741,"user_tz":-330,"elapsed":5956,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"69a4b2d8-d9e0-40ab-d049-08754046d7ed"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |▎                               | 10 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 184 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 204 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 215 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 225 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 235 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 245 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 256 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 266 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 276 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 286 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 296 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 307 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 317 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 327 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 337 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 348 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 358 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 368 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 378 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 389 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 399 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 409 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 419 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 430 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 440 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 450 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 460 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 471 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 481 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 491 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 501 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 512 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 522 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 532 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 542 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 552 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 563 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 573 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 583 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 593 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 604 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 614 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 624 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 634 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 645 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 655 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 665 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 675 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 686 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 696 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 706 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 716 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 727 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 737 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 747 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 757 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 768 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 778 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 788 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 798 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 808 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 819 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 829 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 839 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 849 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 860 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 870 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 880 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 890 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 901 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 911 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 921 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 931 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 942 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 952 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 962 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 972 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 983 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 993 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.0 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.0 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.0 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.1 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.3 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.3 MB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3 MB 7.6 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"lJEnBJ3gHTsQ","executionInfo":{"status":"ok","timestamp":1666590848010,"user_tz":-330,"elapsed":7280,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["2e1033a970b04817be02ed09dbd58fcd","a97c5d06701d4fbd92e2d7af0f53a4e7","a79c4d6f3ff14d5aa948ec36e82fc904","5ea9ef80668642939b6861c6ee92ebf1","e1dc8d62ae784d46a810525569e9200e","cf8a2d5719684138ae4dd13fafab53df","92de8e478b144841bae291ef2a897024","41e0018db87c4c5abb475172c5db2530","def1ae5ca7834922b9062af32e80e2ff","aaed1338cbae4630963d59caf3a4b697","59abb58fbc4146e6bcd8c7a4b563fb7b","16c62e7badb741b09cd4e08ee7ab324f","4070a095b0f443d4897013b931474e85","c50650314249408ea7c405e8d1c511bf","942ce4e0f4e84bca99e076cad4c3807d","1166aef45d9e4350bd84919ec01c6ec5","9bfb3d0300e14b5688b9302bb8d63a7d","00954618803a4649b407a5294d352d32","6eb54771d2914654bc67118b55b33abb","387274ac85f44a3ba9ac43acf44d059b","8a669d5b492842fca6638cdacfab401a","fc6dcb04a2c94ad782ce9a898c41341b"]},"outputId":"01033950-b5af-4f05-ad15-9100a064ae9d"},"source":["import torch\n","# from transformers import BertTokenizer, BertModel\n","\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","# import logging\n","# logging.basicConfig(level=logging.INFO)\n","\n","import matplotlib.pyplot as plt\n","# % matplotlib inline\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","from transformers import AutoTokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\", return_token_type_ids=False)\n","tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/507 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1033a970b04817be02ed09dbd58fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c62e7badb741b09cd4e08ee7ab324f"}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Tlv3VlPnKKHN"},"source":["# 2. Input Formatting\n","Because BERT is a pretrained model that expects input data in a specific format, we will need:\n","\n","1. A **special token, `[SEP]`,** to mark the end of a sentence, or the separation between two sentences\n","2. A **special token, `[CLS]`,** at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is. \n","3. Tokens that conform with the fixed vocabulary used in BERT\n","4. The **Token IDs** for the tokens, from BERT's tokenizer\n","5. **Mask IDs** to indicate which elements in the sequence are tokens and which are padding elements\n","6. **Segment IDs** used to distinguish different sentences\n","7. **Positional Embeddings** used to show token position within the sequence\n","\n","Luckily, the `transformers` interface takes care of all of the above requirements (using the `tokenizer.encode_plus` function). \n","\n","Since this is intended as an introduction to working with BERT, though, we're going to perform these steps in a (mostly) manual way. \n","\n","> *For an example of using `tokenizer.encode_plus`, see the next post on Sentence Classification [here](http://mccormickml.com/2019/07/22/BERT-fine-tuning/).*"]},{"cell_type":"markdown","metadata":{"id":"diVtyCJCurxJ"},"source":["## 2.1. Special Tokens\n","BERT can take as input either one or two sentences, and uses the special token `[SEP]` to differentiate them. The `[CLS]` token always appears at the start of the text, and is specific to classification tasks. \n","\n","Both tokens are *always required*, however, even if we only have one sentence, and even if we are not using BERT for classification. That's how BERT was pre-trained, and so that's what BERT expects to see.\n","\n","**2 Sentence Input**:\n","\n","`[CLS] The man went to the store. [SEP] He bought a gallon of milk.`\n","\n","**1 Sentence Input**:\n","\n","`[CLS] The man went to the store. [SEP]`\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gsyrAwYvBfC"},"source":["## 2.2. Tokenization"]},{"cell_type":"markdown","metadata":{"id":"2WafgQPLAWmo"},"source":["BERT provides its own tokenizer, which we imported above. Let's see how it handles the below sentence."]},{"cell_type":"code","metadata":{"id":"Pg0P9rFxJwwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590848011,"user_tz":-330,"elapsed":93,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"c0dff82e-0607-4aac-e2ef-08029917e14a"},"source":["text = \"अभियुक्त इस मामले में जेल में निरुद्ध है। \"\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","# Tokenize our sentence with the BERT tokenizer.\n","tokenized_text = tokenizer.tokenize(marked_text)\n","\n","# Print out the tokens.\n","print (tokenized_text)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['[CLS]', '▁अभय', 'कत', '▁इस', '▁म', 'मल', '▁म', '▁जल', '▁म', '▁नर', 'द', 'ध', '▁ह', '।', '▁', '[SEP]']\n"]}]},{"cell_type":"markdown","metadata":{"id":"Q51eN4KAkbIJ"},"source":["Notice how the word \"embeddings\" is represented:\n","\n","`['em', '##bed', '##ding', '##s']`\n","\n","The original word has been split into smaller subwords and characters. The two hash signs preceding some of these subwords are just our tokenizer's way to denote that this subword or character is part of a larger word and preceded by another subword. So, for example, the '##bed' token is separate from the 'bed' token; the first is used whenever the subword 'bed' occurs within a larger word and the second is used explicitly for when the standalone token 'thing you sleep on' occurs.\n","\n","Why does it look this way? This is because the BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fits our language data. Since the vocabulary limit size of our BERT tokenizer model is 30,000, the WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on. This vocabulary contains four things:\n","\n","1. Whole words\n","2. Subwords occuring at the front of a word or in isolation (\"em\" as in \"embeddings\" is assigned the same vector as the standalone sequence of characters \"em\" as in \"go get em\" )\n","3. Subwords not at the front of a word, which are preceded by '##' to denote this case\n","4. Individual characters\n","\n","To tokenize a word under this model, the tokenizer first checks if the whole word is in the vocabulary. If not, it tries to break the word into the largest possible subwords contained in the vocabulary, and as a last resort will decompose the word into individual characters. Note that because of this, we can always represent a word as, at the very least, the collection of its individual characters.\n","\n","As a result, rather than assigning out of vocabulary words to a catch-all token like 'OOV' or 'UNK,' words that are not in the vocabulary are decomposed into subword and character tokens that we can then generate embeddings for. \n","\n","So, rather than assigning \"embeddings\" and every other out of vocabulary word to an overloaded unknown vocabulary token, we split it into subword tokens ['em', '##bed', '##ding', '##s'] that will retain some of the contextual meaning of the original word.  We can even average these subword embedding vectors to generate an approximate vector for the original word.\n","\n","\n","(For more information about WordPiece, see the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) and further disucssion in Google's [Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf).)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jp5zXAPBVp82"},"source":["Here are some examples of the tokens contained in our vocabulary. Tokens beginning with two hashes are subwords or individual characters.\n","\n","*For an exploration of the contents of BERT's vocabulary, see [this notebook](https://colab.research.google.com/drive/1fCKIBJ6fgWQ-f6UKs7wDTpNTL9N-Cq9X) I created and the accompanying YouTube video [here](https://youtu.be/zJW57aCBCTk).*"]},{"cell_type":"code","metadata":{"id":"1z1SzuTrqx-7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590848012,"user_tz":-330,"elapsed":84,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"cee5a71a-f8ab-4f47-b37b-77c2a3e45d63"},"source":["list(tokenizer.vocab.keys())[5000:5020]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ਿੰਘ',\n"," '▁ಪಟ್ಟಣದ',\n"," 'row',\n"," '▁வந்துள்ளன',\n"," 'റിനെയും',\n"," '▁রাখুন',\n"," 'ଟ୍ରୋ',\n"," '▁sno',\n"," '▁Senior',\n"," '▁રજૂઆત',\n"," '▁ಕಾರಣದಿಂದಾಗಿ',\n"," '▁സുരക്ഷ',\n"," 'ాజీ',\n"," 'ാടിസ്ഥാനത്തില',\n"," 'రావ్',\n"," '▁സംവിധാനങ്ങള',\n"," 'ಟರ್',\n"," '▁Reddy',\n"," '▁ಪರಾರಿಯಾಗಿದ್ದ',\n"," '▁சொல்றேன்']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"HoF3LC47VgBb"},"source":["After breaking the text into tokens, we then have to convert the sentence from a list of strings to a list of vocabulary indeces.\n","\n","From here on, we'll use the below example sentence, which contains two instances of the word \"bank\" with different meanings."]},{"cell_type":"code","metadata":{"id":"XYjcYJuXoAQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590848014,"user_tz":-330,"elapsed":75,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"ee6b2a62-a338-4145-b1ae-26f2b60d23b2"},"source":["# Define a new example sentence with multiple meanings of the word \"bank\"\n","# text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n","#        \"fishing on the Mississippi river bank.\"\n","\n","# text = \"कल विराट कोहली ने खेल में शतक बनाया \" \\\n","#         \"वह पर्वत बहोत विराट है \"\n","# could not understand that both 'virat' are different\n","\n","# text = 'नदी तेज़ी से बह रही हैं ' \\\n","#         'वह पर्वत बहोत विराट है '\n","\n","text = \"मुझे जल लाकर दो \" \\\n","\"दिया जल रहा है\" \\\n","\"मेरा हाथ जल गया \"\n","\n","# Add the special tokens.\n","marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","# Split the sentence into tokens.\n","tokenized_text = tokenizer.tokenize(marked_text)\n","\n","# Map the token strings to their vocabulary indeces.\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","# Display the words with their indeces.\n","for tup in zip(tokenized_text, indexed_tokens):\n","    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]             2\n","▁                 8\n","मझ           110,854\n","▁जल           2,186\n","▁                 8\n","लकर          65,398\n","▁द            2,155\n","▁द            2,155\n","य             1,134\n","▁जल           2,186\n","▁रह           2,022\n","▁हम             524\n","र               367\n","▁हथ          35,552\n","▁जल           2,186\n","▁ग            2,344\n","य             1,134\n","▁                 8\n","[SEP]             3\n"]}]},{"cell_type":"markdown","source":["**haha it is not so smart**"],"metadata":{"id":"nk2SnISmRlt8"}},{"cell_type":"markdown","metadata":{"id":"if6C_iCULU60"},"source":["## 2.3. Segment ID\n","BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in \"tokenized_text,\" we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence. \n","\n","If you want to process two sentences, assign each word in the first sentence plus the '[SEP]' token a 0, and all tokens of the second sentence a 1."]},{"cell_type":"code","source":[],"metadata":{"id":"t8cDb6gYZxt_","executionInfo":{"status":"ok","timestamp":1666590848015,"user_tz":-330,"elapsed":31,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_jEkVKxJMc0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590848016,"user_tz":-330,"elapsed":31,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"836edf80-ffda-4d5d-8d99-c7027a0cffa9"},"source":["# Mark each of the 22 tokens as belonging to sentence \"1\".\n","segments_ids = [1] * len(tokenized_text)\n","\n","print (segments_ids)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"c-nY9LASLr2L"},"source":["# 3. Extracting Embeddings \n","\n"]},{"cell_type":"markdown","metadata":{"id":"sl-iCj8wMEd5"},"source":["## 3.1. Running BERT on our text\n"]},{"cell_type":"markdown","metadata":{"id":"_Nvaw46mfc8M"},"source":["\n","Next we need to convert our data to torch tensors and call the BERT model. The BERT PyTorch interface requires that the data be in torch tensors rather than Python lists, so we convert the lists here - this does not change the shape or the data.\n"," "]},{"cell_type":"code","metadata":{"id":"E_t4cM6KLc98","executionInfo":{"status":"ok","timestamp":1666590848018,"user_tz":-330,"elapsed":25,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UCIGe0AXfg4Z"},"source":["Calling `from_pretrained` will fetch the model from the internet. When we load the `bert-base-uncased`, we see the definition of the model printed in the logging. The model is a deep neural network with 12 layers! Explaining the layers and their functions is outside the scope of this post, and you can skip over this output for now.\n","\n","model.eval() puts our model in evaluation mode as opposed to training mode. In this case, evaluation mode turns off dropout regularization which is used in training."]},{"cell_type":"code","metadata":{"id":"Mq2PKplWfbFv","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["65f2d95e43ca48f9bcf81cefc1f76346","4d13841ddad94dedbce5197e3b7fbae4","4fc6c3645ad04e41abb088cf46a616ee","455e4abcd6fb40628e04c2adecbe8065","d0fd53a246784cd89e5ada05bff0fd9b","68a46bed388a40639d0d87b9b43de744","9c5e89392905469e953776b9b4d66352","3de637f4bd244def8b616f303ca68fe9","fa27c5508a9742dbad44c4f4b9acf710","f9717ca73c3a42dc9f65f4076690a88c","e17894005f484f349b5f6101e4b4e92e"]},"executionInfo":{"status":"ok","timestamp":1666590856332,"user_tz":-330,"elapsed":8338,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"786554ae-3466-4b81-8822-8a0fe8ff55b3"},"source":["# Load pre-trained model (weights)\n","# model = BertModel.from_pretrained('bert-base-uncased',\n","#                                   output_hidden_states = True, # Whether the model returns all hidden-states.\n","#                                   )\n","\n","from transformers import BertModel # , AdamW, BertConfig\n"," \n","model = BertModel.from_pretrained('ai4bharat/indic-bert', \n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n","                                  )\n","\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n","model.eval()\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/135M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f2d95e43ca48f9bcf81cefc1f76346"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at ai4bharat/indic-bert were not used when initializing BertModel: ['albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'albert.pooler.bias', 'predictions.LayerNorm.bias', 'albert.encoder.embedding_hidden_mapping_in.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'albert.encoder.embedding_hidden_mapping_in.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.embeddings.position_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'predictions.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'albert.embeddings.LayerNorm.bias', 'albert.embeddings.word_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'predictions.decoder.weight', 'sop_classifier.classifier.bias', 'albert.embeddings.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'predictions.bias', 'albert.pooler.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'predictions.LayerNorm.weight', 'sop_classifier.classifier.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'predictions.decoder.bias', 'predictions.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['encoder.layer.8.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.8.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(200000, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"G4Qa5KkkM2Aq"},"source":["Next, let's evaluate BERT on our example text, and fetch the hidden states of the network!\n","\n","*Side note: `torch.no_grad` tells PyTorch not to construct the compute graph during this forward pass (since we won't be running backprop here)--this just reduces memory consumption and speeds things up a little.*\n"]},{"cell_type":"code","metadata":{"id":"nN0QTZwiMzeq","executionInfo":{"status":"ok","timestamp":1666590857275,"user_tz":-330,"elapsed":960,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["# Run the text through BERT, and collect all of the hidden states produced\n","# from all 12 layers. \n","with torch.no_grad():\n","\n","    outputs = model(tokens_tensor, segments_tensors)\n","\n","    # Evaluating the model will return a different number of objects based on \n","    # how it's  configured in the `from_pretrained` call earlier. In this case, \n","    # becase we set `output_hidden_states = True`, the third item will be the \n","    # hidden states from all layers. See the documentation for more details:\n","    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n","    hidden_states = outputs[2]"],"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# outputs"],"metadata":{"id":"36vZi3PMStVA","executionInfo":{"status":"ok","timestamp":1666590857287,"user_tz":-330,"elapsed":115,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["outputs['last_hidden_state'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwPTCkC2SR4R","executionInfo":{"status":"ok","timestamp":1666590857289,"user_tz":-330,"elapsed":114,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"dd5222b5-bb9c-4261-c254-d6e71285dbf3"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 19, 768])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["outputs['pooler_output'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbtdjJ0dS0qo","executionInfo":{"status":"ok","timestamp":1666590857298,"user_tz":-330,"elapsed":113,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"b67a5921-7c4e-4098-9e97-2587efa84c49"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 768])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["type(outputs['hidden_states']), len(outputs['hidden_states'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxeA0-u5TF4Z","executionInfo":{"status":"ok","timestamp":1666590857301,"user_tz":-330,"elapsed":99,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"c72d057c-93ef-432d-840f-4b82eeb3eb7b"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tuple, 13)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["outputs['hidden_states'][0].shape, "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmbMpYgRTXxg","executionInfo":{"status":"ok","timestamp":1666590857303,"user_tz":-330,"elapsed":85,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"efe13c54-b3de-437d-f4c5-0c1a698b4e08"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 19, 768]),)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"UeQNEFbUgMSf"},"source":["## 3.2. Understanding the Output\n"]},{"cell_type":"markdown","metadata":{"id":"HKTlTS_sfuAe"},"source":["\n","The full set of hidden states for this model, stored in the object `hidden_states`, is a little dizzying. This object has four dimensions, in the following order:\n","\n","1. The layer number (13 layers)\n","2. The batch number (1 sentence)\n","3. The word / token number (22 tokens in our sentence)\n","4. The hidden unit / feature number (768 features)\n","\n","Wait, 13 layers? Doesn't BERT only have 12? It's 13 because the first element is the input embeddings, the rest is the outputs of each of BERT's 12 layers. \n","\n","That’s 219,648 unique values just to represent our one sentence! \n","\n","The second dimension, the batch size, is used when submitting multiple sentences to the model at once; here, though, we just have one example sentence."]},{"cell_type":"code","metadata":{"id":"eI_uxiW7eRWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590857306,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"0ecd4c00-466f-41c1-ae71-6c384ef027a8"},"source":["print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n","layer_i = 0\n","\n","print (\"Number of batches:\", len(hidden_states[layer_i]))\n","batch_i = 0\n","\n","print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n","token_i = 0\n","\n","print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of layers: 13   (initial embeddings + 12 BERT layers)\n","Number of batches: 1\n","Number of tokens: 19\n","Number of hidden units: 768\n"]}]},{"cell_type":"markdown","metadata":{"id":"6Uc_S_hmOWe7"},"source":["Let's take a quick look at the range of values for a given layer and token.\n","\n","You'll find that the range is fairly similar for all layers and tokens, with the majority of values falling between \\[-2, 2\\], and a small smattering of values around -10."]},{"cell_type":"code","metadata":{"id":"-UF_OAO-S1sP","colab":{"base_uri":"https://localhost:8080/","height":592},"executionInfo":{"status":"ok","timestamp":1666590858041,"user_tz":-330,"elapsed":791,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"3129c079-792b-4d35-8fee-ab9a06fd9563"},"source":["# For the 5th token in our sentence, select its feature values from layer 5.\n","token_i = 5\n","layer_i = 5\n","vec = hidden_states[layer_i][batch_i][token_i]\n","\n","# Plot the values as a histogram to show their distribution.\n","plt.figure(figsize=(10,10))\n","plt.hist(vec, bins=200)\n","plt.show()"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x720 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"n194RcReDYfw"},"source":["Grouping the values by layer makes sense for the model, but for our purposes we want it grouped by token. \n","\n","Current dimensions:\n","\n","`[# layers, # batches, # tokens, # features]`\n","\n","Desired dimensions:\n","\n","`[# tokens, # layers, # features]`\n","\n","Luckily, PyTorch includes the `permute` function for easily rearranging the dimensions of a tensor. \n","\n","However, the first dimension is currently a Python list! "]},{"cell_type":"code","metadata":{"id":"0CcY_oRwcHlS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858044,"user_tz":-330,"elapsed":143,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"218466c0-8198-4cf3-a0a6-ae00cd76efbe"},"source":["# `hidden_states` is a Python list.\n","print('      Type of hidden_states: ', type(hidden_states))\n","\n","# Each layer in the list is a torch tensor.\n","print('Tensor shape for each layer: ', hidden_states[0].size())"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["      Type of hidden_states:  <class 'tuple'>\n","Tensor shape for each layer:  torch.Size([1, 19, 768])\n"]}]},{"cell_type":"markdown","metadata":{"id":"1yXZjLSke3F0"},"source":["Let's combine the layers to make this one whole big tensor."]},{"cell_type":"code","metadata":{"id":"pTJV8AFFcLbL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858051,"user_tz":-330,"elapsed":129,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"f4281950-07f8-4a31-aea3-58ed6f5653a5"},"source":["# Concatenate the tensors for all layers. We use `stack` here to\n","# create a new dimension in the tensor.\n","token_embeddings = torch.stack(hidden_states, dim=0)\n","\n","token_embeddings.size()"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 1, 19, 768])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"rnBv2TUNhzf4"},"source":["Let's get rid of the \"batches\" dimension since we don't need it."]},{"cell_type":"code","metadata":{"id":"En4JZ41fh6CI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858054,"user_tz":-330,"elapsed":123,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"d288bbce-26a1-44e0-e3ed-89897f3b18e7"},"source":["# Remove dimension 1, the \"batches\".\n","token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","\n","token_embeddings.size()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([13, 19, 768])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"YVzRfvkbe-Yp"},"source":["Finally, we can switch around the \"layers\" and \"tokens\" dimensions with `permute`."]},{"cell_type":"code","metadata":{"id":"AtDVE58cdeYp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858059,"user_tz":-330,"elapsed":118,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"b777af9a-633e-4bb0-f662-2159670448f1"},"source":["# Swap dimensions 0 and 1.\n","token_embeddings = token_embeddings.permute(1,0,2)\n","\n","token_embeddings.size()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([19, 13, 768])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"Ey5RhOQ7NGtz"},"source":["## 3.3. Creating word and sentence vectors from hidden states\n","\n","Now, what do we do with these hidden states? **We would like to get individual vectors for each of our tokens, or perhaps a single vector representation of the whole sentence, but for each token of our input we have 13 separate vectors each of length 768.**\n","\n","**In order to get the individual vectors we will need to combine some of the layer vectors...but which layer or combination of layers provides the best representation?**\n","\n","Unfortunately, there's no single easy answer... Let's try a couple reasonable approaches, though. Afterwards, I'll point you to some helpful resources which look into this question further.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"76TdtFH8NM9q"},"source":["### Word Vectors\n","\n","To give you some examples, let's create word vectors two ways. \n","\n","First, let's **concatenate** the last four layers, giving us a single word vector per token. Each vector will have length `4 x 768 = 3,072`. "]},{"cell_type":"code","metadata":{"id":"pv42h9jANMRf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858062,"user_tz":-330,"elapsed":103,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"ecdc51a9-ab47-4816-e9af-3fe897bef34f"},"source":["# Stores the token vectors, with shape [22 x 3,072]\n","token_vecs_cat = []\n","\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","    \n","    # `token` is a [12 x 768] tensor\n","\n","    # Concatenate the vectors (that is, append them together) from the last \n","    # four layers.\n","    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n","    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n","    \n","    # Use `cat_vec` to represent `token`.\n","    token_vecs_cat.append(cat_vec)\n","\n","print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape is: 19 x 3072\n"]}]},{"cell_type":"markdown","metadata":{"id":"VnWaByfelM-e"},"source":["As an alternative method, let's try creating the word vectors by **summing** together the last four layers."]},{"cell_type":"code","metadata":{"id":"j4DKDtFwiF0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858063,"user_tz":-330,"elapsed":97,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"4f594889-e2ca-4f62-9376-eb024b8404d3"},"source":["# Stores the token vectors, with shape [22 x 768]\n","token_vecs_sum = []\n","\n","# `token_embeddings` is a [22 x 12 x 768] tensor.\n","\n","# For each token in the sentence...\n","for token in token_embeddings:\n","\n","    # `token` is a [12 x 768] tensor\n","\n","    # Sum the vectors from the last four layers.\n","    sum_vec = torch.sum(token[-4:], dim=0)\n","    \n","    # Use `sum_vec` to represent `token`.\n","    token_vecs_sum.append(sum_vec)\n","\n","print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape is: 19 x 768\n"]}]},{"cell_type":"markdown","metadata":{"id":"mQaco6jRLkXn"},"source":["### Sentence Vectors\n"]},{"cell_type":"markdown","metadata":{"id":"uuul6iQqnXT2"},"source":["\n","**To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the second to last hiden layer of each token producing a single 768 length vector.**"]},{"cell_type":"code","metadata":{"id":"Zn0n2S-FWZih","executionInfo":{"status":"ok","timestamp":1666590858070,"user_tz":-330,"elapsed":90,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["# `hidden_states` has shape [13 x 1 x 22 x 768]\n","\n","# `token_vecs` is a tensor with shape [22 x 768]\n","token_vecs = hidden_states[-2][0]\n","\n","# Calculate the average of all 22 token vectors.\n","sentence_embedding = torch.mean(token_vecs, dim=0)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQv0FL8VWadn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666590858072,"user_tz":-330,"elapsed":91,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"3e71c60e-45c8-4ce7-fabe-5ca7cb478df7"},"source":["print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Our final sentence embedding vector of shape: torch.Size([768])\n"]}]},{"cell_type":"code","source":["# sentence_embedding"],"metadata":{"id":"xxE5Yr6xXmPm","executionInfo":{"status":"ok","timestamp":1666590858073,"user_tz":-330,"elapsed":76,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# **Get raw embeddings for each sample**"],"metadata":{"id":"l8tyPnbhXsQI"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"lISuARXcbBUY","executionInfo":{"status":"ok","timestamp":1666590858610,"user_tz":-330,"elapsed":612,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jI34g_Keat3O","executionInfo":{"status":"ok","timestamp":1666590893677,"user_tz":-330,"elapsed":35085,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"7536fbbe-febc-4944-9b61-97bf51892305"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/BERT FINETUNE/40000_rows_stopwords.xlsx')\n","df = df.sample(frac= 0.25, random_state = 42)"],"metadata":{"id":"ZBi6OCGLaszD","executionInfo":{"status":"ok","timestamp":1666590930723,"user_tz":-330,"elapsed":37053,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["df = df[['sentence_without_stopwords', 'facts-and-arguments','decision']] # instead of only facts-and-arguments\n","\n","df.rename(columns = {'sentence_without_stopwords' : 'sentence_wo_stopwords', 'facts-and-arguments' : 'sentence_with_stopwords', 'decision' : 'label'}, \n","          inplace = True\n","          )"],"metadata":{"id":"h0JulvyjbJ85","executionInfo":{"status":"ok","timestamp":1666590930725,"user_tz":-330,"elapsed":102,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["df.head(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"mtxBLaWDbwIX","executionInfo":{"status":"ok","timestamp":1666590930726,"user_tz":-330,"elapsed":100,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"b52c8465-df2a-43bf-b880-ac0a1919fa8c"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                   sentence_wo_stopwords  \\\n","32825                    ['अभियुक्त मामले जेल निरुद्ध ']   \n","14159  ['जमानत प्रार्थना पत्र समर्थन रहमीना पत्नी <ना...   \n","\n","                                 sentence_with_stopwords  label  \n","32825     ['अभियुक्त इस मामले में जेल में निरुद्ध है। ']      1  \n","14159  ['जमानत प्रार्थना पत्र के समर्थन में रहमीना पत...      0  "],"text/html":["\n","  <div id=\"df-df103e0c-aee2-4b98-9966-a07be9e24ed4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_wo_stopwords</th>\n","      <th>sentence_with_stopwords</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>32825</th>\n","      <td>['अभियुक्त मामले जेल निरुद्ध ']</td>\n","      <td>['अभियुक्त इस मामले में जेल में निरुद्ध है। ']</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14159</th>\n","      <td>['जमानत प्रार्थना पत्र समर्थन रहमीना पत्नी &lt;ना...</td>\n","      <td>['जमानत प्रार्थना पत्र के समर्थन में रहमीना पत...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df103e0c-aee2-4b98-9966-a07be9e24ed4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-df103e0c-aee2-4b98-9966-a07be9e24ed4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-df103e0c-aee2-4b98-9966-a07be9e24ed4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["**method 1 truncation**"],"metadata":{"id":"wPhdJDx2jCj7"}},{"cell_type":"code","source":["# def get_sent_emd(text):\n","#     # text = \"मुझे जल लाकर दो \"\n","#     marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","#     # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","#     # input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","#     tokenized_text = tokenizer.tokenize(marked_text)[:512]\n","#     # print(f'tokenised text = {tokenized_text}')\n","#     # print(f'len is {len(tokenized_text)}')\n","\n","#     indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","\n","#     # encoded_dict = tokenizer.encode_plus(\n","#     #                     text,                      # Sentence to encode.\n","#     #                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","#     #                     max_length = 512,           # Pad & truncate all sentences.\n","#     #                     pad_to_max_length = True,\n","#     #                     return_attention_mask = False,   # Construct attn. masks.\n","#     #                     return_tensors = 'pt',     # Return pytorch tensors.\n","#     #                     truncation = True           # Warning message says better to specify directly \n","#     #                )\n","    \n","#     # Add the encoded sentence to the list.    \n","#     # input_ids = encoded_dict['input_ids']\n","#     # print(f'input_ids is {input_ids}, shape is {input_ids.shape}')\n","\n","#     # Convert the lists into tensors.\n","#     # input_ids = torch.cat(input_ids, dim=0)\n","#     # print(f'input_ids = {input_ids}')\n","#     # attention_masks = torch.cat(attention_masks, dim=0)\n","#     # labels = torch.tensor(labels)\n","\n","#     # Print sentence 0, now as a list of IDs.\n","#     # print('Original: ', text)\n","#     # print('Token IDs:', input_ids)\n","\n","#     # tokens_tensor = torch.tensor([input_ids])\n","\n","\n","#     segments_ids = [1] * len(tokenized_text)\n","\n","#     tokens_tensor = torch.tensor([indexed_tokens])\n","#     # print(f'tokens_tensor = {tokens_tensor}')\n","#     segments_tensors = torch.tensor([segments_ids])\n","\n","#     with torch.no_grad():\n","#         outputs = model(tokens_tensor, segments_tensors)\n","#         hidden_states = outputs[2]\n","\n","#     token_vecs = hidden_states[-2][0]\n","#     print\n","#     return torch.mean(token_vecs, dim=0)    "],"metadata":{"id":"dEImPoydXyaf","executionInfo":{"status":"ok","timestamp":1666590930728,"user_tz":-330,"elapsed":55,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["**method2 without truncation**"],"metadata":{"id":"om5hVYBHjHty"}},{"cell_type":"code","source":["from transformers import BertModel # , AdamW, BertConfig\n"," \n","model = BertModel.from_pretrained('ai4bharat/indic-bert', \n","                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n","                                  )\n","\n","# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n","model.eval()"],"metadata":{"id":"dzdgn3sCgQGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sent_emd(text):\n","    # text = \"मुझे जल लाकर दो \"\n","    marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    # input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    tokenized_text = tokenizer.tokenize(marked_text)\n","    # print(f'tokenised text = {tokenized_text}')\n","    # print(f'len is {len(tokenized_text)}')\n","\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","    segments_ids = [1] * len(tokenized_text)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","\n","    print(f'reached here')\n","\n","    with torch.no_grad():\n","        outputs = model(tokens_tensor, segments_tensors)\n","        print(f'got outputs')\n","        hidden_states = outputs[2]\n","\n","\n","    token_vecs = hidden_states[-2][0]\n","    # Calculate the average of all token vectors.\n","    sentence_embedding = torch.mean(token_vecs, dim=0)\n","\n","    return sentence_embedding\n","\n","    # token_embeddings = torch.stack(hidden_states, dim=0)\n","    # token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","    # token_embeddings = token_embeddings.permute(1,0,2)\n","\n","\n","\n","\n","\n","    # encoded_dict = tokenizer.encode_plus(\n","    #                     text,                      # Sentence to encode.\n","    #                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","    #                     max_length = 512,           # Pad & truncate all sentences.\n","    #                     pad_to_max_length = True,\n","    #                     return_attention_mask = False,   # Construct attn. masks.\n","    #                     return_tensors = 'pt',     # Return pytorch tensors.\n","    #                     truncation = True           # Warning message says better to specify directly \n","    #                )\n","    \n","    # # Add the encoded sentence to the list.    \n","    # input_ids = encoded_dict['input_ids']\n","    # print(f'input_ids is {input_ids}, shape is {input_ids.shape}')\n","\n","    # # Convert the lists into tensors.\n","    # # input_ids = torch.cat(input_ids, dim=0)\n","    # # print(f'input_ids = {input_ids}')\n","    # # attention_masks = torch.cat(attention_masks, dim=0)\n","    # # labels = torch.tensor(labels)\n","\n","    # # Print sentence 0, now as a list of IDs.\n","    # print('Original: ', text)\n","    # print('Token IDs:', input_ids)\n","\n","    # # tokens_tensor = torch.tensor([input_ids])\n","    # tokens_tensor = input_ids\n","\n","\n","    # segments_ids = [1] * len(tokenized_text)\n","\n","    # # tokens_tensor = torch.tensor([indexed_tokens])\n","    # print(f'tokens_tensor = {tokens_tensor}')\n","    # segments_tensors = torch.tensor([segments_ids])\n","\n","    # with torch.no_grad():\n","    #     outputs = model(tokens_tensor, segments_tensors)\n","    #     hidden_states = outputs[2]\n","\n","    # token_vecs = hidden_states[-2][0]\n","    # print\n","    # return torch.mean(token_vecs, dim=0)    "],"metadata":{"id":"jc77_AidjHb4","executionInfo":{"status":"ok","timestamp":1666591094207,"user_tz":-330,"elapsed":553,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["marked_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RSSdKatWlaeT","executionInfo":{"status":"ok","timestamp":1666591341575,"user_tz":-330,"elapsed":520,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"9aa56381-56ab-4975-898e-e0fc71a2573d"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'[CLS] मुझे जल लाकर दो दिया जल रहा हैमेरा हाथ जल गया  [SEP]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"W7gB5syplcT0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# df['sentence_wo_stopwords_emb'] = df['sentence_wo_stopwords'].apply(get_sent_emd)\n","aa = df['sentence_wo_stopwords'][1:3].apply(get_sent_emd)"],"metadata":{"id":"YwpEWAvtb1n_","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"ok","timestamp":1666591168721,"user_tz":-330,"elapsed":15,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"95915c85-f40a-4fd5-c715-333fd798cf42"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["reached here\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-38-b4eb99f01449>\u001b[0m in \u001b[0;36mget_sent_emd\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'got outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (714) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 714].  Tensor sizes: [1, 512]"]}]},{"cell_type":"code","source":["aa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wpS8_fQGknQ3","executionInfo":{"status":"ok","timestamp":1666591130438,"user_tz":-330,"elapsed":14,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"b275d394-6a75-4ac1-9c3b-1d0615ab1326"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32825    [tensor(-0.7480), tensor(-0.2068), tensor(0.03...\n","Name: sentence_wo_stopwords, dtype: object"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["len(aa.values[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTv3MjuJkol7","executionInfo":{"status":"ok","timestamp":1666591149160,"user_tz":-330,"elapsed":9,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}},"outputId":"3a3ed8a1-113a-420f-8ec3-00849e5a6a41"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[],"metadata":{"id":"JlfmBdJhkoiH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","embs = df['sentence_wo_stopwords'][:50].apply(get_sent_emd)"],"metadata":{"id":"ssaXvmULoQWE","executionInfo":{"status":"aborted","timestamp":1666590936723,"user_tz":-330,"elapsed":20,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","embs = df['sentence_wo_stopwords'][:50].apply(get_sent_emd, ram = True)"],"metadata":{"id":"8dQnWAZWi-4l","executionInfo":{"status":"aborted","timestamp":1666590936724,"user_tz":-330,"elapsed":20,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embs"],"metadata":{"id":"XRaNXxXYoeB7","executionInfo":{"status":"aborted","timestamp":1666590936726,"user_tz":-330,"elapsed":21,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(embs.values[1]), embs.values[1].shape"],"metadata":{"id":"eXhB7Rmgom4h","executionInfo":{"status":"aborted","timestamp":1666590937242,"user_tz":-330,"elapsed":60,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","embs = df['sentence_wo_stopwords'][:50].map(get_sent_emd)"],"metadata":{"id":"niHRiKDTh0Pd","executionInfo":{"status":"aborted","timestamp":1666590937245,"user_tz":-330,"elapsed":61,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%time\n","# df['applymap'] = df.applymap(get_sent_emd).loc[:50, 'sentence_wo_stopwords']"],"metadata":{"id":"5tK2Hi0oiieV","executionInfo":{"status":"aborted","timestamp":1666590937247,"user_tz":-330,"elapsed":63,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q2Q6kCB0iiZa","executionInfo":{"status":"aborted","timestamp":1666590937249,"user_tz":-330,"elapsed":64,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BaKbYN3ziiQD","executionInfo":{"status":"aborted","timestamp":1666590937250,"user_tz":-330,"elapsed":64,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['sentence_wo_stopwords'] = \n","'[CLS]' + df['sentence_wo_stopwords'] + '[SEP]'\n","# \"[CLS] \" + text + \" [SEP]\""],"metadata":{"id":"NTCFDfw6j9J1","executionInfo":{"status":"aborted","timestamp":1666590937256,"user_tz":-330,"elapsed":69,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(df['sentence_wo_stopwords'][1])"],"metadata":{"id":"PzCNxSKSkWmk","executionInfo":{"status":"aborted","timestamp":1666590937257,"user_tz":-330,"elapsed":69,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sentence_wo_stopwords'][1]"],"metadata":{"id":"D9NV3jTxkgmn","executionInfo":{"status":"aborted","timestamp":1666590937258,"user_tz":-330,"elapsed":69,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gg"],"metadata":{"id":"cAQKmfSVjY4c","executionInfo":{"status":"aborted","timestamp":1666590937259,"user_tz":-330,"elapsed":69,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sentence_wo_stopwords_emb'] = df['sentence_wo_stopwords'].apply(get_sent_emd)"],"metadata":{"id":"FbxJDQBXjHUt","executionInfo":{"status":"aborted","timestamp":1666590937261,"user_tz":-330,"elapsed":70,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"anB7PXRujHOh","executionInfo":{"status":"aborted","timestamp":1666590937269,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_sent_emd(\"मुझे जल लाकर दो \")"],"metadata":{"id":"JfpD-qYLXyWM","executionInfo":{"status":"aborted","timestamp":1666590937271,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JGXtKBj3XyP8","executionInfo":{"status":"aborted","timestamp":1666590937272,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tMGWUpHyXyL4","executionInfo":{"status":"aborted","timestamp":1666590937273,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqYcrAipfE3E"},"source":["## 3.4. Confirming contextually dependent vectors\n","\n","To confirm that the value of these vectors are in fact contextually dependent, let's look at the different instances of the word \"bank\" in our example sentence:\n","\n","\"After stealing money from the **bank vault**, the **bank robber** was seen fishing on the Mississippi **river bank**.\"\n","\n","Let's find the index of those three instances of the word \"bank\" in the example sentence."]},{"cell_type":"code","metadata":{"id":"DNiRsEh9cmWz","executionInfo":{"status":"aborted","timestamp":1666590937274,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["for i, token_str in enumerate(tokenized_text):\n","  print (i, token_str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEhBIA5RlS8-"},"source":["-- They are at 6, 10, and 19.\n","**For us they are at 2, 6 and 11. 6 and 11 are similar in meaning of jal**\n","\n","For this analysis, we'll use the word vectors that we created by summing the last four layers.\n","\n","We can try printing out their vectors to compare them."]},{"cell_type":"code","metadata":{"id":"tBa6vRHknSkv","executionInfo":{"status":"aborted","timestamp":1666590937275,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["# print('First 5 vector values for each instance of \"bank\".')\n","# print('')\n","# print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n","# print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n","# print(\"river bank   \", str(token_vecs_sum[19][:5]))\n","\n","print(\"जल लाकर  \", str(token_vecs_sum[2][:5]))\n","print(\"दिया जल \", str(token_vecs_sum[6][:5]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ca2TCQ_G7SM3"},"source":["We can see that the values differ, but let's calculate the cosine similarity between the vectors to make a more precise comparison."]},{"cell_type":"code","metadata":{"id":"eYXUwiG0yhBS","executionInfo":{"status":"aborted","timestamp":1666590937276,"user_tz":-330,"elapsed":77,"user":{"displayName":"ed19b048 Girhepuje Sahil Satishkumar","userId":"09412542214202402728"}}},"source":["from scipy.spatial.distance import cosine\n","\n","# Calculate the cosine similarity between the word bank \n","# in \"bank robber\" vs \"river bank\" (different meanings).\n","diff_bank = 1 - cosine(token_vecs_sum[2], token_vecs_sum[6])\n","\n","# Calculate the cosine similarity between the word bank\n","# in \"bank robber\" vs \"bank vault\" (same meaning).\n","same_bank = 1 - cosine(token_vecs_sum[6], token_vecs_sum[11])\n","\n","print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n","print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N7jroXfKspe_"},"source":["This looks pretty good! -- **not for us**"]},{"cell_type":"markdown","metadata":{"id":"orjhWUJgmxo5"},"source":["## 3.5. Pooling Strategy & Layer Choice"]},{"cell_type":"markdown","metadata":{"id":"f1CI97kNn8dD"},"source":["Below are a couple additional resources for exploring this topic."]},{"cell_type":"markdown","metadata":{"id":"P3D5qnRNmq5_"},"source":["**BERT Authors**\n","\n","The BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n","\n","(Image from [Jay Allamar](http://jalammar.github.io/illustrated-bert/)'s blog)\n","\n","\n","![alt text](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n","\n","While concatenation of the last four layers produced the best results on this specific task, many of the other methods come in a close second and in general it is advisable to test different versions for your specific application: results may vary.\n","\n","This is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information. \n"]},{"cell_type":"markdown","metadata":{"id":"m7_CVgejm5pr"},"source":["**Han Xiao's BERT-as-service**\n","\n","Han Xiao created an open-source project named [bert-as-service](https://github.com/hanxiao/bert-as-service) on GitHub which is intended to create word embeddings for your text using BERT. Han experimented with different approaches to combining these embeddings, and shared some conclusions and rationale on the [FAQ page](https://github.com/hanxiao/bert-as-service#speech_balloon-faq) of the project. \n","\n","`bert-as-service`, by default, uses the outputs from the **second-to-last layer** of the model. \n","\n","I would summarize Han's perspective by the following:\n","\n","1. The embeddings start out in the first layer as having no contextual information (i.e., the meaning of the initial 'bank' embedding isn't specific to river bank or financial bank).\n","2. As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n","3. As you approach the final layer, however, you start picking up information that is specific to BERT's pre-training tasks (the \"Masked Language Model\" (MLM) and \"Next Sentence Prediction\" (NSP)). \n","    * What we want is embeddings that encode the word meaning well... \n","    * BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP). \n","4. The second-to-last layer is what Han settled on as a reasonable sweet-spot.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ONLJ36JfPuqf"},"source":["# 4. Appendix\n"]},{"cell_type":"markdown","metadata":{"id":"jdw7cLJWMr_Y"},"source":["## 4.1. Special tokens\n"]},{"cell_type":"markdown","metadata":{"id":"Jyx2kQxbnHbM"},"source":["\n","It should be noted that although the `[CLS]` acts as an \"aggregate representation\" for classification tasks, this is not the best choice for a high quality sentence embedding vector. [According to](https://github.com/google-research/bert/issues/164) BERT author Jacob Devlin: \"*I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations*.\"\n","\n","(However, the [CLS] token does become meaningful if the model has been fine-tuned, where the last hidden layer of this token is used as the \"sentence vector\" for sequence classification.)\n"]},{"cell_type":"markdown","metadata":{"id":"EbS8_z6XMuTJ"},"source":["\n","## 4.2. Out of vocabulary words\n","\n","For **out of vocabulary words** that are composed of multiple sentence and character-level embeddings, there is a further issue of how best to recover this embedding. Averaging the embeddings is the most straightforward solution (one that is relied upon in similar embedding models with subword vocabularies like fasttext), but summation of subword embeddings and simply taking the last token embedding (remember that the vectors are context sensitive) are acceptable alternative strategies.\n"]},{"cell_type":"markdown","metadata":{"id":"BokW7CAgMxCB"},"source":["\n","## 4.3. Similarity metrics\n","\n","It is worth noting that word-level **similarity comparisons** are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This allows wonderful things like polysemy so that e.g. your representation encodes river \"bank\" and not a financial institution \"bank\",  but makes direct word-to-word similarity comparisons less valuable. However, for sentence embeddings similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs since many similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n"]},{"cell_type":"markdown","metadata":{"id":"0unZ2xh4QDap"},"source":["## 4.4. Implementations\n","\n","You can use the code in this notebook as the foundation of your own application to extract BERT features from text. However, official [tensorflow](https://github.com/google-research/bert/blob/master/extract_features.py) and well-regarded [pytorch](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/extract_features.py) implementations already exist that do this for you.  Additionally, [bert-as-a-service](https://github.com/hanxiao/bert-as-service) is an excellent tool designed specifically for running this task with high performance, and is the one I would recommend for production applications. The author has taken great care in the tool's implementation and provides excellent documentation (some of which was used to help create this tutorial) to help users understand the more nuanced details the user faces, like resource management and pooling strategy."]},{"cell_type":"markdown","metadata":{"id":"OhbZxbKRxMvM"},"source":["## Cite\n","Chris McCormick and Nick Ryan. (2019, May 14). *BERT Word Embeddings Tutorial*. Retrieved from http://www.mccormickml.com\n"]}]}